{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CUDA - GPU - ćwiczenia\n",
        "**Paweł Kruczkiewicz**\n",
        "\n",
        "\n",
        "TODO\n",
        "  - pliki wrzucić tak jak trzeba, żeby się poprawnie kompilowało\n",
        "  - podzielić logicznie cellki w notebooku\n",
        "  - dodać odpowiednie kompilacje\n",
        "  - napisać wnioski\n",
        "\n",
        "Estimated time < 2h\n",
        "\n",
        "## Przygotowanie środowiska wykonawczego"
      ],
      "metadata": {
        "id": "UcC4W1dw1NXe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50A-J95903qv",
        "outputId": "94fca272-69ea-4be4-be4b-1eb7bc509960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfjlneZe1C8N",
        "outputId": "4caac4a7-c6b9-4523-fffe-204d9d965556"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-f0mhknr3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-f0mhknr3\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4287 sha256=10d2b23ef240a00ca42dc12f6a7fe8f97890ffbebd3eab6e2218690e8956ed59\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-j84d7k_t/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8Pxy3JX1559",
        "outputId": "bc5b8ac5-04d2-4e10-e70a-a30f1f0e9636"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The nvcc_plugin extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc_plugin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwHrKdoGlcKT",
        "outputId": "ea8326ec-d736-4e69-b940-ab186cb14908"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jun 13 14:15:20 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <iostream>\n",
        "    int\n",
        "    main()\n",
        "{\n",
        "    std::cout << \"Welcome To Informatyka na WIET\\n\";\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieerR5NF1_us",
        "outputId": "6ec54cf5-577e-4c20-fa84-2e8b85fcaa9a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome To Informatyka na WIET\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <cstdio>\n",
        "#include <iostream>\n",
        "\n",
        "\tusing namespace std;\n",
        "\n",
        "__global__ void maxi(int* a, int* b, int n)\n",
        "{\n",
        "\tint block = 256 * blockIdx.x;\n",
        "\tint max = 0;\n",
        "\n",
        "\tfor (int i = block; i < min(256 + block, n); i++) {\n",
        "\n",
        "\t\tif (max < a[i]) {\n",
        "\t\t\tmax = a[i];\n",
        "\t\t}\n",
        "\t}\n",
        "\tb[blockIdx.x] = max;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "\n",
        "\tint n;\n",
        "\tn = 3 >> 2;\n",
        "\tint a[n];\n",
        "\n",
        "\tfor (int i = 0; i < n; i++) {\n",
        "\t\ta[i] = rand() % n;\n",
        "\t\tcout << a[i] << \"\\t\";\n",
        "\t}\n",
        "\n",
        "\tcudaEvent_t start, end;\n",
        "\tint *ad, *bd;\n",
        "\tint size = n * sizeof(int);\n",
        "\tcudaMalloc(&ad, size);\n",
        "\tcudaMemcpy(ad, a, size, cudaMemcpyHostToDevice);\n",
        "\tint grids = ceil(n * 1.0f / 256.0f);\n",
        "\tcudaMalloc(&bd, grids * sizeof(int));\n",
        "\n",
        "\tdim3 grid(grids, 1);\n",
        "\tdim3 block(1, 1);\n",
        "\n",
        "\tcudaEventCreate(&start);\n",
        "\tcudaEventCreate(&end);\n",
        "\tcudaEventRecord(start);\n",
        "\n",
        "\twhile (n > 1) {\n",
        "\t\tmaxi<<<grids, block>>>(ad, bd, n);\n",
        "\t\tn = ceil(n * 1.0f / 256.0f);\n",
        "\t\tcudaMemcpy(ad, bd, n * sizeof(int), cudaMemcpyDeviceToDevice);\n",
        "\t}\n",
        "\n",
        "\tcudaEventRecord(end);\n",
        "\tcudaEventSynchronize(end);\n",
        "\n",
        "\tfloat time = 0;\n",
        "\tcudaEventElapsedTime(&time, start, end);\n",
        "\n",
        "\tint ans[2];\n",
        "\tcudaMemcpy(ans, ad, 4, cudaMemcpyDeviceToHost);\n",
        "\n",
        "\tcout << \"The maximum element is : \" << ans[0] << endl;\n",
        "\n",
        "\tcout << \"The time required : \";\n",
        "\tcout << time << endl;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiJLXrJWJ8uV",
        "outputId": "7a1bf9ff-ab2e-4d9c-a19c-6dd1dcea1295"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The maximum element is : 0\n",
            "The time required : 0.002688\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Przygotowanie środeowiska pod laby"
      ],
      "metadata": {
        "id": "nmcSJh82o2sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pkrucz00/parallel-programming.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1mYLDTepNh-",
        "outputId": "2bec3946-d41f-48f5-b736-1c28cde0cf59"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'parallel-programming'...\n",
            "remote: Enumerating objects: 346, done.\u001b[K\n",
            "remote: Counting objects: 100% (145/145), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 346 (delta 74), reused 108 (delta 70), pack-reused 201\u001b[K\n",
            "Receiving objects: 100% (346/346), 40.20 MiB | 21.56 MiB/s, done.\n",
            "Resolving deltas: 100% (166/166), done.\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd parallel-programming/\n",
        "!git checkout gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMt7Lm7oHv86",
        "outputId": "276250e5-b8c2-457c-82f1-9ed21a9c0d9c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/parallel-programming\n",
            "Branch 'gpu' set up to track remote branch 'gpu' from 'origin'.\n",
            "Switched to a new branch 'gpu'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!nvidia-smi\n",
        "!free -g"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L4ZZXt-pxtm",
        "outputId": "97aefa58-9d0b-4191-9c76-0ba95fbfc936"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jun 13 14:19:06 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:             12           0           9           0           2          11\n",
            "Swap:             0           0           0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gpu/CUDA-Lab02"
      ],
      "metadata": {
        "id": "Q7vOlEjOtBqD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0356ab19-718b-402c-e026-c69ee011e792"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'gpu/CUDA-Lab02'\n",
            "/content/parallel-programming/gpu/CUDA-Lab02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOGRI1q2p5Q8",
        "outputId": "b69a19d3-d3ea-4671-99ff-10c512fb4bef"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1  2  CUDA-Lab02  data\tsprawozdanie-3.pdf  sprawozdanie.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Ćwiczenie 1\n",
        "\n",
        "### Reduction"
      ],
      "metadata": {
        "id": "1mEZOLvV5SQv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"1 Reduction\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RNhR2DjIUom",
        "outputId": "24df8243-6024-4daf-fddc-e14b54c5d16f"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/parallel-programming/gpu/CUDA-Lab02/1 Reduction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfPXflqtIcTn",
        "outputId": "8f0796b0-3975-4ee9-b55f-f6dfdeb79c49"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "analyser.bat\treduction_global.cpp\t    reduction_shared.cpp\n",
            "exception.h\treduction_global_gpu.bat    reduction_shared_kernel.cu\n",
            "helper_timer.h\treduction_global_kernel.cu\n",
            "profiler.bat\treduction.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Globalna**"
      ],
      "metadata": {
        "id": "Q1wwgwa2bzts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o reduction_global_gpu reduction_global.cpp reduction_global_kernel.cu -arch=sm_50 -allow-unsupported-compiler -I.\n"
      ],
      "metadata": {
        "id": "eMq9auvMIiW9"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 777 reduction_global_gpu\n",
        "!./reduction_global_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vItJzCj_NKPI",
        "outputId": "21251157-da66-47ec-fc3b-31e1e06b0497"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time= 14.468 msec, bandwidth= 4.638505 GB/s\n",
            "host: 0.996007, device 0.996007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./reduction_global_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCKNdgISNYHK",
        "outputId": "09dd08f0-c3df-45de-fb0e-7bf78961cf8a"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==24861== NVPROF is profiling process 24861, command: ./reduction_global_gpu\n",
            "Time= 14.640 msec, bandwidth= 4.584061 GB/s\n",
            "host: 0.996007, device 0.996007\n",
            "==24861== Profiling application: ./reduction_global_gpu\n",
            "==24861== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   95.23%  1.40321s      2424  578.88us  451.42us  790.87us  global_reduction_kernel(float*, float*, int, int)\n",
            "                    3.74%  55.136ms       100  551.36us  543.51us  557.59us  [CUDA memcpy DtoD]\n",
            "                    1.02%  15.077ms         1  15.077ms  15.077ms  15.077ms  [CUDA memcpy HtoD]\n",
            "                    0.00%  1.7600us         1  1.7600us  1.7600us  1.7600us  [CUDA memcpy DtoH]\n",
            "      API calls:   81.86%  1.44307s       101  14.288ms  6.3930us  34.923ms  cudaDeviceSynchronize\n",
            "                   16.06%  283.21ms         2  141.61ms  227.16us  282.99ms  cudaMalloc\n",
            "                    1.04%  18.368ms       102  180.08us  20.505us  15.284ms  cudaMemcpy\n",
            "                    0.94%  16.561ms      2424  6.8320us  5.0530us  51.977us  cudaLaunchKernel\n",
            "                    0.08%  1.4919ms         2  745.96us  321.81us  1.1701ms  cudaFree\n",
            "                    0.01%  174.76us       101  1.7300us     263ns  70.105us  cuDeviceGetAttribute\n",
            "                    0.00%  31.035us         1  31.035us  31.035us  31.035us  cuDeviceGetName\n",
            "                    0.00%  8.7780us         1  8.7780us  8.7780us  8.7780us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.3830us         3     794ns     365ns  1.6380us  cuDeviceGetCount\n",
            "                    0.00%     991ns         2     495ns     344ns     647ns  cuDeviceGet\n",
            "                    0.00%     596ns         1     596ns     596ns     596ns  cuDeviceTotalMem\n",
            "                    0.00%     460ns         1     460ns     460ns     460ns  cuModuleGetLoadingMode\n",
            "                    0.00%     393ns         1     393ns     393ns     393ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uGud2KMWbuBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof --analysis-metrics -o reduction_global_gpu.nvvp -f  ./reduction_global_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyX7j5OONwfq",
        "outputId": "6568ae4c-d9f1-48b4-bcf5-d617729a3b89"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Warning: Skipping profiling on device 0 since profiling is not supported on devices with compute capability 7.5 and higher.\n",
            "                  Use NVIDIA Nsight Compute for GPU profiling and NVIDIA Nsight Systems for GPU tracing and CPU sampling.\n",
            "                  Refer https://developer.nvidia.com/tools-overview for more details.\n",
            "\n",
            "======== Warning: The option --aggregate-mode on has no effect. The --aggregate-mode <on|off> option applies to --events and --metrics options that follow it.\n",
            "======== Warning: The option --aggregate-mode off has no effect. The --aggregate-mode <on|off> option applies to --events and --metrics options that follow it.\n",
            "==23995== NVPROF is profiling process 23995, command: ./reduction_global_gpu\n",
            "Time= 14.424 msec, bandwidth= 4.652674 GB/s\n",
            "host: 0.996007, device 0.996007\n",
            "==23995== Generated result file: /content/parallel-programming/gpu/CUDA-Lab02/1 Reduction/reduction_global_gpu.nvvp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDzyACUQbit1",
        "outputId": "53fab899-9ab4-4be9-8b1c-174bf65060af"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: nvvp: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F8zhdyBIaat",
        "outputId": "16df83df-e376-4c4b-c373-99e08d56854b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/parallel-programming/gpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Ćwiczenie 2"
      ],
      "metadata": {
        "id": "n1FH3e4eqMET"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"2 Warp Divergence\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnDG1jtXSKAG",
        "outputId": "554667b6-47e8-4fe5-e7c7-cc7a783c3544"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/parallel-programming/gpu/CUDA-Lab02/2 Warp Divergence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o reduction_interleaving reduction.cpp reduction_kernel_interleaving.cu -arch=sm_50 -allow-unsupported-compiler -I."
      ],
      "metadata": {
        "id": "5tWGvs14SVEL"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o reduction_sequential reduction.cpp reduction_kernel_sequential.cu -arch=sm_50 -allow-unsupported-compiler -I."
      ],
      "metadata": {
        "id": "QHgibO8jUGZH"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 744 reduction_interleaving\n",
        "!chmod 744 reduction_sequential\n",
        "\n",
        "!./reduction_interleaving\n",
        "!./reduction_sequential"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bZa85VsUUj7",
        "outputId": "2dc74cf4-8ac7-4dab-e7f2-eef5df0d0b98"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time= 3.025 msec, bandwidth= 22.186216 GB/s\n",
            "host: 0.996007, device 0.996007\n",
            "Time= 2.490 msec, bandwidth= 26.953516 GB/s\n",
            "host: 0.996007, device 0.996007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "FLPC9U4t8-cD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c08f7a-d6bf-41db-9f8b-6ef485f4e9a3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/parallel-programming/gpu/CUDA-Lab02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "evWRY4ux1K_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include<stdio.h>\n",
        "#include<stdlib.h>\n",
        "\n",
        "#define BLOCK_SIZE 32\n",
        "\n",
        "__global__ void matrix_transpose_naive(int *input, int *output, int n) {\n",
        "\n",
        "\tint indexX = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\tint indexY = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "\tint index = indexY * n + indexX;\n",
        "\tint transposedIndex = indexX * n + indexY;\n",
        "\n",
        "    // this has discoalesced global memory store\n",
        "\toutput[transposedIndex] = input[index];\n",
        "\n",
        "\t// this has discoalesced global memore load\n",
        "\t// output[index] = input[transposedIndex];\n",
        "}\n",
        "\n",
        "__global__ void matrix_transpose_shared(int *input, int *output, int n) {\n",
        "\n",
        "\t__shared__ int sharedMemory [BLOCK_SIZE] [BLOCK_SIZE];\n",
        "\n",
        "\t// global index\n",
        "\tint indexX = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\tint indexY = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "\n",
        "\t// transposed global memory index\n",
        "\tint tindexX = threadIdx.x + blockIdx.y * blockDim.x;\n",
        "\tint tindexY = threadIdx.y + blockIdx.x * blockDim.y;\n",
        "\n",
        "\t// local index\n",
        "\tint localIndexX = threadIdx.x;\n",
        "\tint localIndexY = threadIdx.y;\n",
        "\n",
        "\tint index = indexY * n + indexX;\n",
        "\tint transposedIndex = tindexY * n + tindexX;\n",
        "\n",
        "\t// reading from global memory in coalesed manner and performing tanspose in shared memory\n",
        "\tsharedMemory[localIndexX][localIndexY] = input[index];\n",
        "\n",
        "\t__syncthreads();\n",
        "\n",
        "\t// writing into global memory in coalesed fashion via transposed data in shared memory\n",
        "\toutput[transposedIndex] = sharedMemory[localIndexY][localIndexX];\n",
        "}\n",
        "\n",
        "//basically just fills the array with index.\n",
        "void fill_array(int *data, int n) {\n",
        "\tfor(int idx=0;idx<(n*n);idx++)\n",
        "\t\tdata[idx] = idx;\n",
        "}\n",
        "\n",
        "void print_output(int *a, int *b, int n) {\n",
        "\tprintf(\"\\n Original Matrix::\\n\");\n",
        "\tfor(int idx=0;idx<(n*n);idx++) {\n",
        "\t\tif(idx%n == 0)\n",
        "\t\t\tprintf(\"\\n\");\n",
        "\t\tprintf(\" %d \",  a[idx]);\n",
        "\t}\n",
        "\tprintf(\"\\n Transposed Matrix::\\n\");\n",
        "\tfor(int idx=0;idx<(n*n);idx++) {\n",
        "\t\tif(idx%n == 0)\n",
        "\t\t\tprintf(\"\\n\");\n",
        "\t\tprintf(\" %d \",  b[idx]);\n",
        "\t}\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "\tint *a, *b;\n",
        "        int *d_a, *d_b; // device copies of a, b, c\n",
        "\n",
        "\tcudaEvent_t start, end;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&end);\n",
        "\n",
        "  for (int n = 1028; n <= 10280; n += 1028){\n",
        "    int size = n * n *sizeof(int);\n",
        "\n",
        "    // Alloc space for host copies of a, b, c and setup input values\n",
        "    a = (int *)malloc(size); fill_array(a, n);\n",
        "    b = (int *)malloc(size);\n",
        "\n",
        "    // Alloc space for device copies of a, b, c\n",
        "    cudaMalloc((void **)&d_a, size);\n",
        "    cudaMalloc((void **)&d_b, size);\n",
        "\n",
        "    // Copy inputs to device\n",
        "    cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 blockSize(BLOCK_SIZE,BLOCK_SIZE,1);\n",
        "    dim3 gridSize(n/BLOCK_SIZE,n/BLOCK_SIZE,1);\n",
        "\n",
        "    for (int i = 0; i < 5; i++){\n",
        "    cudaEventRecord(start);\n",
        "    // matrix_transpose_naive<<<gridSize,blockSize>>>(d_a,d_b,n);\n",
        "    matrix_transpose_shared<<<gridSize,blockSize>>>(d_a,d_b,n);\n",
        "    cudaEventRecord(end);\n",
        "    cudaEventSynchronize(end);\n",
        "\n",
        "    float time = 0;\n",
        "    cudaEventElapsedTime(&time, start, end);\n",
        "    printf(\"%d,%f,%d\\n\", n, time, i);\n",
        "}\n",
        "    free(a);\n",
        "    free(b);\n",
        "      cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "  }\n",
        "\n",
        "\treturn 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xYOguqeKI2G",
        "outputId": "6de99c0e-d526-4ab1-b42f-154075931747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1028,0.141408,0\n",
            "1028,0.145376,1\n",
            "1028,0.139424,2\n",
            "1028,0.139296,3\n",
            "1028,0.139264,4\n",
            "2056,0.536928,0\n",
            "2056,0.528640,1\n",
            "2056,0.525984,2\n",
            "2056,0.528352,3\n",
            "2056,0.527360,4\n",
            "3084,1.195296,0\n",
            "3084,1.194752,1\n",
            "3084,1.190944,2\n",
            "3084,1.189920,3\n",
            "3084,1.193536,4\n",
            "4112,2.079040,0\n",
            "4112,2.077408,1\n",
            "4112,2.069216,2\n",
            "4112,2.078080,3\n",
            "4112,2.073216,4\n",
            "5140,3.305408,0\n",
            "5140,3.300992,1\n",
            "5140,3.303456,2\n",
            "5140,3.301920,3\n",
            "5140,3.300384,4\n",
            "6168,4.689760,0\n",
            "6168,4.685184,1\n",
            "6168,4.690880,2\n",
            "6168,4.700512,3\n",
            "6168,4.691936,4\n",
            "7196,6.504000,0\n",
            "7196,6.507616,1\n",
            "7196,6.503136,2\n",
            "7196,6.498848,3\n",
            "7196,6.500480,4\n",
            "8224,8.243040,0\n",
            "8224,8.241408,1\n",
            "8224,8.251936,2\n",
            "8224,8.252288,3\n",
            "8224,8.240960,4\n",
            "9252,10.611904,0\n",
            "9252,10.605984,1\n",
            "9252,10.608608,2\n",
            "9252,10.605952,3\n",
            "9252,10.600768,4\n",
            "10280,11.835904,0\n",
            "10280,11.829248,1\n",
            "10280,11.840032,2\n",
            "10280,11.839360,3\n",
            "10280,11.839456,4\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a5AqfHqnKZXJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}